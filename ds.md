```

Data Structures and Algorithms: Key Concepts
Array: A collection of elements stored at contiguous memory locations, accessed by indices.

Linked List: A sequential collection of nodes where each node contains data and a reference to the next node.

Stack: A Last-In-First-Out (LIFO) structure where elements are added and removed from the top.

Queue: A First-In-First-Out (FIFO) structure where elements are added at the rear and removed from the front.

Deque: A double-ended queue where elements can be added or removed from both ends.

Hash Table: A data structure that maps keys to values using a hash function for fast access.

Tree: A hierarchical structure consisting of nodes, with one root node and potentially many levels of sub-nodes (children).

Binary Tree: A tree in which each node has at most two children.

Binary Search Tree (BST): A binary tree where the left child contains values less than the parent, and the right child contains values greater than the parent.

Heap: A special tree-based structure that satisfies the heap property (min-heap or max-heap).

Graph: A collection of nodes (vertices) connected by edges, which may be directed or undirected.

Depth-First Search (DFS): An algorithm for traversing graphs by exploring as far as possible along each branch before backtracking.

Breadth-First Search (BFS): An algorithm for traversing graphs level by level, visiting all neighbors of a node before moving to the next level.

Sorting Algorithms: Techniques to arrange elements in a particular order. Examples include Bubble Sort, Merge Sort, and Quick Sort.

Searching Algorithms: Methods to find specific elements within a data structure. Examples include Linear Search and Binary Search.

Dynamic Programming: A method for solving problems by breaking them down into overlapping subproblems and storing results to avoid redundant computations.

Greedy Algorithm: An approach that makes the locally optimal choice at each step to find the global optimum.

Recursion: A technique where a function calls itself to solve smaller instances of the same problem.

Backtracking: A method for solving problems incrementally by trying possible solutions and abandoning them if they don‚Äôt work.

Divide and Conquer: An algorithm design paradigm that solves a problem by dividing it into smaller subproblems, solving each independently, and combining their results.

Time Complexity:

It measures the amount of time an algorithm takes to complete as a function of the input size ùëõ
Expressed using Big-O notation (e.g., O(1), O(n), O(log n)).
Purpose: To predict the runtime performance as the input size grows.
Example:

O(1): Constant time, independent of input size.
O(n): Linear time, grows proportionally with input size.
O(n¬≤): Quadratic time, grows exponentially as input size doubles.
Space Complexity:

It measures the amount of memory an algorithm uses as a function of the input size ùëõ

Includes both the space needed for input data and auxiliary (temporary) storage.
Example:

O(1): Constant space, memory usage doesn‚Äôt grow with input size.
O(n): Linear space, memory usage grows proportionally with input size.
Why They Matter:
Time Complexity helps in understanding how fast an algorithm runs.
Space Complexity indicates how efficiently it uses memory, crucial for systems with limited resources.

```
